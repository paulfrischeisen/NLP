{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/2560px-Scikit_learn_logo_small.svg.png\"  width=\"100\">\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg\"  width=\"640\">\n",
    "\n",
    "https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:04.748214900Z",
     "start_time": "2025-03-21T10:25:03.262130900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Textvektorisierer werden typischerweise mit vielen Dokumenten befüllt, der Einfachheit\n",
    "halber steigen wir jetzt mit einem Dokument, das nur aus einem Satz besteht, ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:08.575969100Z",
     "start_time": "2025-03-21T10:25:08.302418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Einem reichen Manne, dem wurde seine Frau krank, und als sie fühlte, daß ihr Ende herankam, rief sie ihr einziges Töchterlein zu sich ans Bett und sprach: \"Liebes Kind, bleibe fromm und gut, so wird dir der liebe Gott immer beistehen, und ich will vom Himmel auf dich herabblicken, und will um dich sein.\"']\n",
      "Einem reichen Manne, dem wurde seine Frau krank, und als sie fühlte, daß ihr Ende herankam, rief sie ihr einziges Töchterlein zu sich ans Bett und sprach: \"Liebes Kind, bleibe fromm und gut, so wird dir der liebe Gott immer beistehen, und ich will vom Himmel auf dich herabblicken, und will um dich sein.\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(docs)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(docs[\u001B[32m0\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdocs\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[31mIndexError\u001B[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "docs=[\n",
    "    'Einem reichen Manne, dem wurde seine Frau krank, und als sie fühlte, daß ihr Ende herankam, rief sie ihr einziges Töchterlein zu sich ans Bett und sprach: \"Liebes Kind, bleibe fromm und gut, so wird dir der liebe Gott immer beistehen, und ich will vom Himmel auf dich herabblicken, und will um dich sein.\"'\n",
    "]\n",
    "print(docs)\n",
    "print(docs[0])\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:11.908498Z",
     "start_time": "2025-03-21T10:25:11.892369100Z"
    }
   },
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:14.433266500Z",
     "start_time": "2025-03-21T10:25:14.414711600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _VectorizerMixin.build_preprocessor of CountVectorizer()>\n",
      "functools.partial(<function _preprocess at 0x0000023D1D5D0A40>, accent_function=None, lower=True)\n"
     ]
    }
   ],
   "source": [
    "print(cv.build_preprocessor)\n",
    "print(cv.build_preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:19.789193500Z",
     "start_time": "2025-03-21T10:25:19.759180100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einem reichen manne, dem wurde seine frau krank, und als sie fühlte, daß ihr ende herankam, rief sie ihr einziges töchterlein zu sich ans bett und sprach: \"liebes kind, bleibe fromm und gut, so wird dir der liebe gott immer beistehen, und ich will vom himmel auf dich herabblicken, und will um dich sein.\"\n"
     ]
    }
   ],
   "source": [
    "print(cv.build_preprocessor()(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:39.900855700Z",
     "start_time": "2025-03-21T10:25:39.879089200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Einem', 'reichen', 'Manne', 'dem', 'wurde', 'seine', 'Frau', 'krank', 'und', 'als', 'sie', 'fühlte', 'daß', 'ihr', 'Ende', 'herankam', 'rief', 'sie', 'ihr', 'einziges', 'Töchterlein', 'zu', 'sich', 'ans', 'Bett', 'und', 'sprach', 'Liebes', 'Kind', 'bleibe', 'fromm', 'und', 'gut', 'so', 'wird', 'dir', 'der', 'liebe', 'Gott', 'immer', 'beistehen', 'und', 'ich', 'will', 'vom', 'Himmel', 'auf', 'dich', 'herabblicken', 'und', 'will', 'um', 'dich', 'sein']\n"
     ]
    }
   ],
   "source": [
    "print(cv.build_tokenizer()(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:25:45.108929700Z",
     "start_time": "2025-03-21T10:25:45.103882200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['einem', 'reichen', 'manne', 'dem', 'wurde', 'seine', 'frau', 'krank', 'und', 'als', 'sie', 'fühlte', 'daß', 'ihr', 'ende', 'herankam', 'rief', 'sie', 'ihr', 'einziges', 'töchterlein', 'zu', 'sich', 'ans', 'bett', 'und', 'sprach', 'liebes', 'kind', 'bleibe', 'fromm', 'und', 'gut', 'so', 'wird', 'dir', 'der', 'liebe', 'gott', 'immer', 'beistehen', 'und', 'ich', 'will', 'vom', 'himmel', 'auf', 'dich', 'herabblicken', 'und', 'will', 'um', 'dich', 'sein']\n"
     ]
    }
   ],
   "source": [
    "print(cv.build_analyzer()(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:26:05.364605400Z",
     "start_time": "2025-03-21T10:26:05.349890200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['als' 'ans' 'auf' 'beistehen' 'bett' 'bleibe' 'daß' 'dem' 'der' 'dich'\n",
      " 'dir' 'einem' 'einziges' 'ende' 'frau' 'fromm' 'fühlte' 'gott' 'gut'\n",
      " 'herabblicken' 'herankam' 'himmel' 'ich' 'ihr' 'immer' 'kind' 'krank'\n",
      " 'liebe' 'liebes' 'manne' 'reichen' 'rief' 'sein' 'seine' 'sich' 'sie'\n",
      " 'so' 'sprach' 'töchterlein' 'um' 'und' 'vom' 'will' 'wird' 'wurde' 'zu']\n",
      "{'einem': 11, 'reichen': 30, 'manne': 29, 'dem': 7, 'wurde': 44, 'seine': 33, 'frau': 14, 'krank': 26, 'und': 40, 'als': 0, 'sie': 35, 'fühlte': 16, 'daß': 6, 'ihr': 23, 'ende': 13, 'herankam': 20, 'rief': 31, 'einziges': 12, 'töchterlein': 38, 'zu': 45, 'sich': 34, 'ans': 1, 'bett': 4, 'sprach': 37, 'liebes': 28, 'kind': 25, 'bleibe': 5, 'fromm': 15, 'gut': 18, 'so': 36, 'wird': 43, 'dir': 10, 'der': 8, 'liebe': 27, 'gott': 17, 'immer': 24, 'beistehen': 3, 'ich': 22, 'will': 42, 'vom': 41, 'himmel': 21, 'auf': 2, 'dich': 9, 'herabblicken': 19, 'um': 39, 'sein': 32}\n"
     ]
    }
   ],
   "source": [
    "word_count_vector=cv.fit_transform(docs)\n",
    "print(cv.get_feature_names_out())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha: Es erfolgte ein Tokenisierung in Kleinbuchstaben, Satzzeichen sind eliminiert,\n",
    "    deutsche Umlaute bleiben erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe: Testen Sie mit mehreren Sätzen und mehreren Dokumenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\n",
    "    #...\n",
    "]\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schreiben wir einen eigenen Tokenisier. Die Schnittstelle platt gesagt:\n",
    "- String rein\n",
    "- Array aus Tokens raus.\n",
    "Was macht dieser Tokenisiere anders? Er konvertiert in Großbuchstaben und ersetzt die\n",
    "deutschen Umlaute (hatten wir schon mal...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:28:29.936423900Z",
     "start_time": "2025-03-21T10:28:29.914677100Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(txt):\n",
    "    s=txt.split()\n",
    "    for i in range(len(s)):\n",
    "        s[i] = s[i].upper()\n",
    "        s[i] = s[i].replace(\"Ä\",\"AE\")\n",
    "        s[i] = s[i].replace(\"Ö\",\"OE\")\n",
    "        s[i] = s[i].replace(\"Ü\",\"UE\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Über Tokenisierung wurde schon viel geredet, wir hatten in der Vorlesung auch schon\n",
    " bessere über reguläre Ausdrücke, Sie erinnern sich? Verbessern Sie bitte den Tokenisierer!!!)\n",
    " \n",
    " Testen wir den Tokenisiere kurz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:28:44.023559500Z",
     "start_time": "2025-03-21T10:28:44.009078100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['DAS', 'DASS', 'IST', 'EIN', 'OE']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer(\"das daß ist ein ö\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und jetzt Tokenisieren wir wieder das Aschenputtel-Märchen, aber mit unserem eigenen\n",
    "Tokenisierer. Wie? Wir übergeben die Funktion über den Parameter tokenizer an den\n",
    "Countvectorizer-Konstuktor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:29:15.161322900Z",
     "start_time": "2025-03-21T10:29:15.149435500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"LIEBES' 'ALS' 'ANS' 'AUF' 'BEISTEHEN,' 'BETT' 'BLEIBE' 'DASS' 'DEM'\n",
      " 'DER' 'DICH' 'DIR' 'EINEM' 'EINZIGES' 'ENDE' 'FRAU' 'FROMM' 'FUEHLTE,'\n",
      " 'GOTT' 'GUT,' 'HERABBLICKEN,' 'HERANKAM,' 'HIMMEL' 'ICH' 'IHR' 'IMMER'\n",
      " 'KIND,' 'KRANK,' 'LIEBE' 'MANNE,' 'REICHEN' 'RIEF' 'SEIN.\"' 'SEINE'\n",
      " 'SICH' 'SIE' 'SO' 'SPRACH:' 'TOECHTERLEIN' 'UM' 'UND' 'VOM' 'WILL' 'WIRD'\n",
      " 'WURDE' 'ZU']\n",
      "{'EINEM': 12, 'REICHEN': 30, 'MANNE,': 29, 'DEM': 8, 'WURDE': 44, 'SEINE': 33, 'FRAU': 15, 'KRANK,': 27, 'UND': 40, 'ALS': 1, 'SIE': 35, 'FUEHLTE,': 17, 'DASS': 7, 'IHR': 24, 'ENDE': 14, 'HERANKAM,': 21, 'RIEF': 31, 'EINZIGES': 13, 'TOECHTERLEIN': 38, 'ZU': 45, 'SICH': 34, 'ANS': 2, 'BETT': 5, 'SPRACH:': 37, '\"LIEBES': 0, 'KIND,': 26, 'BLEIBE': 6, 'FROMM': 16, 'GUT,': 19, 'SO': 36, 'WIRD': 43, 'DIR': 11, 'DER': 9, 'LIEBE': 28, 'GOTT': 18, 'IMMER': 25, 'BEISTEHEN,': 4, 'ICH': 23, 'WILL': 42, 'VOM': 41, 'HIMMEL': 22, 'AUF': 3, 'DICH': 10, 'HERABBLICKEN,': 20, 'UM': 39, 'SEIN.\"': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulf\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(tokenizer=my_tokenizer)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "print(cv.get_feature_names_out())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wird jetzt der Minitutiv von \"Tochter\" ausgegeben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Information-Retrieval sind Stoipwörter irrelevant - Sie erinnern sich anZipf und Luhn? Sie stören bei der Vektorisierung und beim Retrieval. Leider\n",
    "hat scikit-learn keine deutsche Stopwortliste - daher jetzzt kurz ein englisches Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:33:36.210771700Z",
     "start_time": "2025-03-21T10:33:36.172346600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n",
      "{'test': 0}\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(stop_words='english')\n",
    "word_count_vector=cv.fit_transform([\"This is a test\"])\n",
    "print(cv.get_feature_names_out())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem Satz bleibt nicht viel übrig...\n",
    "\n",
    "Aber wir können unsere eigene Stopwortliste übergeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:33:59.067942600Z",
     "start_time": "2025-03-21T10:33:59.064238900Z"
    }
   },
   "outputs": [],
   "source": [
    "my_stop_words=[\"der\",\"die\",\"das\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:34:35.166911200Z",
     "start_time": "2025-03-21T10:34:35.133924100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beste' 'daran' 'schlusspunkt' 'war']\n",
      "{'beste': 0, 'daran': 1, 'war': 3, 'schlusspunkt': 2}\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(stop_words=my_stop_words)\n",
    "word_count_vector=cv.fit_transform([\"Das beste daran war der Schlusspunkt.\"])\n",
    "print(cv.get_feature_names_out())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:31:42.980286100Z",
     "start_time": "2025-03-21T10:31:42.968018400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'abc', 'abcd', 'a1', 'kund', 'in', 'enn']\n"
     ]
    }
   ],
   "source": [
    "docs = [\"a ab abc abcd a1 kund(in)enn\"]\n",
    "print(cv.build_tokenizer()(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
